{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np #import numpy library\n",
    "import pandas as pd #import pandas library\n",
    "import matplotlib.pyplot as plt #import matplot library\n",
    "%matplotlib inline\n",
    "import nltk.classify.util\n",
    "from astropy.table import Table, Column\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455000, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>helpScore</th>\n",
       "      <th>helpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138806</td>\n",
       "      <td>138807</td>\n",
       "      <td>B000E63LME</td>\n",
       "      <td>A1CQGW1AOD0LF2</td>\n",
       "      <td>Alena K. \"Alena\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1294185600</td>\n",
       "      <td>Not as pictured.</td>\n",
       "      <td>I was looking forward to try cranberry apple f...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>469680</td>\n",
       "      <td>469681</td>\n",
       "      <td>B004ZIH4KM</td>\n",
       "      <td>A37S7U1OX2MCWI</td>\n",
       "      <td>Becky Cole</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1349740800</td>\n",
       "      <td>seeds</td>\n",
       "      <td>TY for everything.  The seeds arrived quickly,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238202</td>\n",
       "      <td>238203</td>\n",
       "      <td>B003ZXE9QA</td>\n",
       "      <td>A2OM6G73E64EQ9</td>\n",
       "      <td>jeff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1329264000</td>\n",
       "      <td>I'm addicted!</td>\n",
       "      <td>I've finally found the best cereal in the worl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>485307</td>\n",
       "      <td>485308</td>\n",
       "      <td>B001RVFERK</td>\n",
       "      <td>A25W349EE97NBK</td>\n",
       "      <td>Tangent4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1248307200</td>\n",
       "      <td>I wanted to love these...</td>\n",
       "      <td>I originally bought these chips because I'd he...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375283</td>\n",
       "      <td>375284</td>\n",
       "      <td>B000OQZNTS</td>\n",
       "      <td>A3CPPW0HUC07YS</td>\n",
       "      <td>Amy Nicolai</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1333238400</td>\n",
       "      <td>Excellent chamomile tea</td>\n",
       "      <td>Really excellent tea, flowers are visible in t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Id   ProductId          UserId       ProfileName  \\\n",
       "0      138806  138807  B000E63LME  A1CQGW1AOD0LF2  Alena K. \"Alena\"   \n",
       "1      469680  469681  B004ZIH4KM  A37S7U1OX2MCWI        Becky Cole   \n",
       "2      238202  238203  B003ZXE9QA  A2OM6G73E64EQ9              jeff   \n",
       "3      485307  485308  B001RVFERK  A25W349EE97NBK          Tangent4   \n",
       "4      375283  375284  B000OQZNTS  A3CPPW0HUC07YS       Amy Nicolai   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       2      2  1294185600   \n",
       "1                     0                       0      5  1349740800   \n",
       "2                     0                       0      5  1329264000   \n",
       "3                     1                       1      4  1248307200   \n",
       "4                     0                       0      5  1333238400   \n",
       "\n",
       "                     Summary  \\\n",
       "0           Not as pictured.   \n",
       "1                      seeds   \n",
       "2              I'm addicted!   \n",
       "3  I wanted to love these...   \n",
       "4    Excellent chamomile tea   \n",
       "\n",
       "                                                Text  helpScore helpful  \n",
       "0  I was looking forward to try cranberry apple f...        0.5   False  \n",
       "1  TY for everything.  The seeds arrived quickly,...        NaN   False  \n",
       "2  I've finally found the best cereal in the worl...        NaN   False  \n",
       "3  I originally bought these chips because I'd he...        1.0   False  \n",
       "4  Really excellent tea, flowers are visible in t...        NaN   False  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Amazon.csv') #attach Amazon data to a var called data\n",
    "# can also include a http addy in there.... \n",
    "print(data.shape) #print data rows and columns\n",
    "data.head(5) #limit data to 5 rows including a header row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#smData = data[0:1000] #make a variabe for data subset limited to first 1000 \n",
    "#smData.shape #show how many rows and columns we have\n",
    "#print(smData.shape) #print data rows and columns\n",
    "#smData.head(5) #limit data to 5 rows including a header row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize Bag of Words from review text; as sparse matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(min_df=1, ngram_range=(3,3))\n",
    "X = tv.fit_transform(data.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # vectorize Bag of Words from review text; as sparse matrix\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# hv = HashingVectorizer(n_features=2 ** 17, non_negative=True)\n",
    "# X = hv.transform(data.Text)\n",
    "# X.shape\n",
    "# (455000, 131072)\n",
    "\n",
    "# CRASHES!!!\n",
    "# # vectorize Bag of Words from review text; as sparse matrix\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tv = TfidfVectorizer(min_df=1, ngram_range=(2,17))\n",
    "# X = tv.fit_transform(data.Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455000, 131072)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert summary NaNs into usable format -- thank you Andy!\n",
    "data.Summary = data.Summary.fillna('none')\n",
    "# data.ProfileName = data.ProfileName.fillna('none')\n",
    "# data.Score = data.Score.fillna('none')\n",
    "\n",
    "XSummary = hv.transform(data.Summary)\n",
    "XSummary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert unix timestamp to datetime -- thanks Tyler!\n",
    "data['DateTime'] = pd.to_datetime(data['Time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features extraction function -- thanks elena\n",
    "def featuresExtract(data):\n",
    "    '''Takes as an argument the original dataframe\n",
    "       Extracts number of unique users and products\n",
    "       as well as number of repeat users and product to user ratio\n",
    "       Returns Pandas dataframe with new features\n",
    "    '''\n",
    "\n",
    "    # how many reviews for the same product in test set - products with more reviews may have more helpful comments as they are more popular\n",
    "    nReviewsProducts = data.groupby(['ProductId'])['Id'].transform('count')\n",
    "\n",
    "    # ratio of n reviews to product\n",
    "    nReviewsRatio = 1 / data.groupby(['ProductId'])['UserId'].transform('count')\n",
    "\n",
    "    # n repeat users - hypothesis is that people who comment more may provide more helpful reviews\n",
    "    nRepeatUsers = data.groupby(['UserId'])['ProductId'].transform('count')\n",
    "\n",
    "    # n unique users\n",
    "    nUniqueUsers = data.groupby(['ProductId'])['UserId'].transform('count')\n",
    "    \n",
    "    # first review\n",
    "    first = data['Time'].rank(ascending=1)\n",
    "    \n",
    "    #data and time\n",
    "\n",
    "    # all these Times are midnight of a day\n",
    "\n",
    "    # find day of week (0-6, Mon-Sun)\n",
    "    day = data['DateTime'].dt.weekday\n",
    "    month = data['DateTime'].dt.month\n",
    "    year = data['DateTime'].dt.year\n",
    "\n",
    "\n",
    "    names = np.array([ 'nReviewsProducts', 'nReviewsRatio', 'nRepeatUsers', 'nUniqueUsers', 'first_review', 'day', 'month', 'year' ])\n",
    "    result = np.array([ nReviewsProducts, nReviewsRatio, nRepeatUsers, nUniqueUsers, first, day, month, year ]).T\n",
    "    myFeatures = pd.DataFrame( result, columns = names )\n",
    "\n",
    "\n",
    "    return myFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_analysis = featuresExtract(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features extraction function -- thanks elena\n",
    "def textProcessor(reviewText):\n",
    "    '''Takes as an argument the text of the reviews\n",
    "       Performs a series of transformations\n",
    "       Returns Pandas dataframe with ten new features\n",
    "    '''\n",
    "\n",
    "    # count number of words in review\n",
    "    nWords = [len(w.split()) for w in reviewText]\n",
    "\n",
    "    # count number of characters in review\n",
    "    nChar = [len(c) for c in reviewText]\n",
    "\n",
    "    # words to characters ratio\n",
    "    wordCharRatio = [ w/c for w,c in zip( map(float, nWords), map(float, nChar) ) ]\n",
    "\n",
    "    # number of uppercase words\n",
    "    nUpper =  [ np.sum( [w.isupper() for w in word.split()] ) for word in reviewText ]\n",
    "\n",
    "    # uppercase to word ratio\n",
    "    upperRatio = np.array(nUpper) / np.array(nWords, dtype=float)\n",
    "\n",
    "    # longest word\n",
    "    exclude = string.punctuation\n",
    "    words = [ word.split() for word in reviewText ]\n",
    "    noPunct = [ [''.join(k for k in c if k not in exclude) for c in w ] for w in words]\n",
    "    longestWord = [ np.max( [len(w) for w in word] ) for word in noPunct ]\n",
    "\n",
    "    # average word length\n",
    "    avgWordLen = [ np.mean( [len(w) for w in word.split()] ) for word in reviewText ]\n",
    "\n",
    "    # punctuation count\n",
    "    punctCount = [ np.sum([c.count(p) for p in string.punctuation]) for c in reviewText ]\n",
    "\n",
    "    # punctuation ratio to character ration\n",
    "    punctRatio = [ p/c for p,c in zip( map(float, punctCount), map(float, nChar) ) ]\n",
    "\n",
    "    # exclamation point count\n",
    "    exclamationPoint = [e.count(\"!\") for e in reviewText]\n",
    "\n",
    "    names = np.array([ 'nWords', 'nChar', 'wordCharRatio', 'nUpper', 'upperRatio', 'longestWord', 'avgWordLen', 'exclamationPoint', 'punctCount', 'punctRatio' ])\n",
    "    result = np.array([ nWords, nChar, wordCharRatio, nUpper, upperRatio, longestWord, avgWordLen, exclamationPoint, punctCount, punctRatio ]).T\n",
    "\n",
    "    myFeatures = pd.DataFrame( result, columns = names )\n",
    "    # print(myFeatures)\n",
    "    # print(myFeatures.shape)\n",
    "    # display(myFeatures)\n",
    "\n",
    "    return myFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(data.Summary)\n",
    "# print(data.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_data = textProcessor(data.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_data = textProcessor(data.Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.20000000e+01   2.06000000e+02   1.55339806e-01 ...,   2.00000000e+00\n",
      "    1.00000000e+00   2.01100000e+03]\n",
      " [  2.60000000e+01   1.25000000e+02   2.08000000e-01 ...,   1.00000000e+00\n",
      "    1.00000000e+01   2.01200000e+03]\n",
      " [  1.24000000e+02   6.78000000e+02   1.82890855e-01 ...,   2.00000000e+00\n",
      "    2.00000000e+00   2.01200000e+03]\n",
      " ..., \n",
      " [  4.40000000e+01   2.83000000e+02   1.55477032e-01 ...,   0.00000000e+00\n",
      "    8.00000000e+00   2.01100000e+03]\n",
      " [  4.60000000e+01   2.66000000e+02   1.72932331e-01 ...,   1.00000000e+00\n",
      "    6.00000000e+00   2.01000000e+03]\n",
      " [  5.30000000e+01   2.75000000e+02   1.92727273e-01 ...,   3.00000000e+00\n",
      "    5.00000000e+00   2.01200000e+03]]\n"
     ]
    }
   ],
   "source": [
    "X_counts = np.concatenate((text_data, summary_data, user_analysis), axis = 1)\n",
    "\n",
    "print(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# features from Amazon.csv to add to feature set\n",
    "# data['reviewLen'] = data['Text'].str.len() #captures the amount of strings present in the review (i.e. review length)\n",
    "# Features pulled from review txt\n",
    "data['hasEP'] = data.Text.str.contains('!') #captures the presences of an exclaimation point\n",
    "data['hasSemi'] = data.Text.str.contains(';') #captures the presences of a ;\n",
    "data['hasColon'] = data.Text.str.contains(':') #captures the presences of a :\n",
    "data['hasAt'] = data.Text.str.contains('@') #captures the presences of a @\n",
    "data['hasDolla'] = data.Text.str.contains('$') #captures the presences of a $\n",
    "data['hasNumb'] = data.Text.str.contains('#') #captures the presences of a #\n",
    "data['hasAnd'] = data.Text.str.contains('&') #captures the presences of a &\n",
    "data['hasQuestion'] = data.Text.str.contains('\\?') #captures the presences of a ?\n",
    "data['hasAsterick'] = data.Text.str.contains('\\*') #captures the presences of a *\n",
    "data['hasFparenthesis'] = data.Text.str.contains('\\(') #captures the presences of a (\n",
    "data['hasBparenthesis'] = data.Text.str.contains('\\)') #captures the presences of a )\n",
    "data['epCount'] = data.Text.str.count('!')\n",
    "data['semiCount'] = data.Text.str.count(':')\n",
    "data['atCount'] = data.Text.str.count('@')\n",
    "data['dollaCount'] = data.Text.str.count('$')\n",
    "data['numbCount'] = data.Text.str.count('#')\n",
    "data['andCount'] = data.Text.str.count('&')\n",
    "data['questionCount'] = data.Text.str.count('\\?')\n",
    "data['asterickCount'] = data.Text.str.count('\\*')\n",
    "data['fParenthesisCount'] = data.Text.str.count('\\(')\n",
    "data['bParenthesisCount'] = data.Text.str.count('\\)')\n",
    "\n",
    "# # add punctuation count and ratio of punctuation to words -- thank you jon!\n",
    "# data['punctuationCount'] = ddata.Text.str.count('[.,!;:()/\\?-@#$&]') #captures total common punctuation count\n",
    "# data['punctuationToWords'] = data['punctuationCount'] / data['reviewLen'] #captures ratio of punctuation to words\n",
    "\n",
    "# # find repeating reviews\n",
    "data['repeatTxt'] = data.duplicated('Text')\n",
    "\n",
    "# # add vars from profile name \n",
    "data['nameNumb'] = data['ProfileName'].str.contains('#') #captures profile name with a number sign\n",
    "data['nameQuestion'] = data['ProfileName'].str.contains('\\?')#captures profile name with a question mark\n",
    "data['nameAsterick'] = data['ProfileName'].str.contains('\\*') #captures profile name with an asterick\n",
    "data['nameAnd'] = data['ProfileName'].str.contains('&') #captures the presences of a &\n",
    "data['nameSemi'] = data['ProfileName'].str.contains(';') #captures the presences of a ;\n",
    "data['nameColon'] = data['ProfileName'].str.contains(':') #captures the presences of a :\n",
    "data['nameAt'] = data['ProfileName'].str.contains('@') #captures the presences of a @\n",
    "data['nameDolla'] = data['ProfileName'].str.contains('$') #captures the presences of a $\n",
    "\n",
    "# # finding repeating profile names \n",
    "# data['repeatProfile'] = data.duplicated('ProfileName')\n",
    "\n",
    "# # Features pulled from summary txt\n",
    "# data['summaryLen'] = data['Summary'].str.len() #captures the amount of strings present in the review (i.e. review length)\n",
    "data['summaryEP'] = data['Summary'].str.contains('!') #captures the presences of an exclaimation point\n",
    "data['summarySemi'] = data['Summary'].str.contains(';') #captures the presences of a ;\n",
    "data['summaryColon'] = data['Summary'].str.contains(':') #captures the presences of a :\n",
    "data['summaryAt'] = data['Summary'].str.contains('@') #captures the presences of a @\n",
    "data['summaryDolla'] = data['Summary'].str.contains('$') #captures the presences of a $\n",
    "data['summaryNumb'] = data['Summary'].str.contains('#') #captures the presences of a #\n",
    "data['summaryAnd'] = data['Summary'].str.contains('&') #captures the presences of a &\n",
    "data['summaryQuestion'] = data['Summary'].str.contains('\\?') #captures the presences of a ?\n",
    "data['summaryAsterick'] = data['Summary'].str.contains('\\*') #captures the presences of a *\n",
    "data['summaryFparenthesis'] = data['Summary'].str.contains('\\(') #captures the presences of a (\n",
    "data['summaryBparenthesis'] = data['Summary'].str.contains('\\)') #captures the presences of a )\n",
    "data['summaryEpCount'] = data['Summary'].str.count('!')\n",
    "data['summarySemiCount'] = data['Summary'].str.count(':')\n",
    "data['summaryAtCount'] = data['Summary'].str.count('@')\n",
    "data['summaryDollaCount'] = data['Summary'].str.count('$')\n",
    "data['summaryNumbCount'] = data['Summary'].str.count('#')\n",
    "data['summaryAndCount'] = data['Summary'].str.count('&')\n",
    "data['summaryQuestionCount'] = data['Summary'].str.count('\\?')\n",
    "data['summaryAsterickCount'] = data['Summary'].str.count('\\*')\n",
    "data['summaryFParenthesisCount'] = data['Summary'].str.count('\\(')\n",
    "data['summaryBParenthesisCount'] = data['Summary'].str.count('\\)')\n",
    "\n",
    "# # add punctuation count and ratio of punctuation to words -- thank you jon!\n",
    "# data['summaryPunctuationCount'] = data['Summary'].str.count('[.,!;:()/\\?-@#$&]') #captures total common punctuation count\n",
    "# data['summaryPunctuationToWords'] = data['punctuationCount'] / data['summaryLen'] #captures ratio of punctuation to words\n",
    "\n",
    "# # find repeating reviews\n",
    "data['summaryRepeatTxt'] = data.duplicated('Summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taking values and making vectors\n",
    "# XScore = data.score.values.reshape(data.shape[0], 1)\n",
    "# XreviewLen = data.iloc[:, 13].values.reshape(data.shape[0], 1)\n",
    "XhasEP = data.hasEP.values.reshape(data.shape[0], 1)\n",
    "XhasSemi = data.hasSemi.values.reshape(data.shape[0], 1)\n",
    "XhasColon = data.hasColon.values.reshape(data.shape[0], 1)\n",
    "hasAt = data.hasAt.values.reshape(data.shape[0], 1)\n",
    "hasDolla = data.hasDolla.values.reshape(data.shape[0], 1)\n",
    "hasNumb = data.hasNumb.values.reshape(data.shape[0], 1)\n",
    "hasAnd = data.hasAnd.values.reshape(data.shape[0], 1)\n",
    "hasQuestion = data.hasQuestion.values.reshape(data.shape[0], 1)\n",
    "hasAsterick = data.hasAsterick.values.reshape(data.shape[0], 1)\n",
    "hasFparenthesis = data.hasFparenthesis.values.reshape(data.shape[0], 1)\n",
    "hasBparenthesis = data.hasBparenthesis.values.reshape(data.shape[0], 1)\n",
    "epCount = data.epCount.values.reshape(data.shape[0], 1)\n",
    "semiCount = data.semiCount.values.reshape(data.shape[0], 1)\n",
    "atCount = data.atCount.values.reshape(data.shape[0], 1)\n",
    "dollaCount = data.dollaCount.values.reshape(data.shape[0], 1)\n",
    "numbCount = data.numbCount.values.reshape(data.shape[0], 1)\n",
    "andCount = data.andCount.values.reshape(data.shape[0], 1)\n",
    "questionCount = data.questionCount.values.reshape(data.shape[0], 1)\n",
    "asterickCount = data.asterickCount.values.reshape(data.shape[0], 1)\n",
    "fParenthesisCount = data.fParenthesisCount.values.reshape(data.shape[0], 1)\n",
    "bParenthesisCount = data.bParenthesisCount.values.reshape(data.shape[0], 1)\n",
    "# punctuationCount = data.iloc[:, 35].values.reshape(data.shape[0], 1)\n",
    "# punctuationToWords = data.iloc[:, 36].values.reshape(data.shape[0], 1)\n",
    "repeatTxt = data.repeatTxt.values.reshape(data.shape[0], 1)\n",
    "nameNumb = data.nameNumb.values.reshape(data.shape[0], 1)\n",
    "nameQuestion = data.nameQuestion.values.reshape(data.shape[0], 1)\n",
    "nameAsterick = data.nameAsterick.values.reshape(data.shape[0], 1)\n",
    "nameAnd = data.nameAnd.values.reshape(data.shape[0], 1)\n",
    "nameSemi = data.nameSemi.values.reshape(data.shape[0], 1)\n",
    "nameColon = data.nameColon.values.reshape(data.shape[0], 1)\n",
    "nameAt = data.nameAt.values.reshape(data.shape[0], 1)\n",
    "nameDolla = data.nameDolla.values.reshape(data.shape[0], 1)\n",
    "# repeatProfile = data.repeatProfile.values.reshape(data.shape[0], 1)\n",
    "# summaryLen = data.iloc[:, 47].values.reshape(data.shape[0], 1)\n",
    "summaryEP = data.summaryEP.values.reshape(data.shape[0], 1)\n",
    "summarySemi = data.summarySemi.values.reshape(data.shape[0], 1)\n",
    "summaryColon = data.summaryColon.values.reshape(data.shape[0], 1)\n",
    "summaryAt = data.summaryAt.values.reshape(data.shape[0], 1)\n",
    "summaryDolla = data.summaryDolla.values.reshape(data.shape[0], 1)\n",
    "summaryNumb = data.summaryNumb.values.reshape(data.shape[0], 1)\n",
    "summaryAnd = data.summaryAnd.values.reshape(data.shape[0], 1)\n",
    "summaryQuestion = data.summaryQuestion.values.reshape(data.shape[0], 1)\n",
    "summaryAsterick = data.summaryAsterick.values.reshape(data.shape[0], 1)\n",
    "summaryFparenthesis = data.summaryFparenthesis.values.reshape(data.shape[0], 1)\n",
    "summaryBparenthesis = data.summaryBparenthesis.values.reshape(data.shape[0], 1)\n",
    "summaryEpCount = data.summaryEpCount.values.reshape(data.shape[0], 1)\n",
    "summarySemiCount = data.summarySemiCount.values.reshape(data.shape[0], 1)\n",
    "summaryAtCount = data.summaryAtCount.values.reshape(data.shape[0], 1)\n",
    "summaryDollaCount = data.summaryDollaCount.values.reshape(data.shape[0], 1)\n",
    "summaryNumbCount = data.summaryNumbCount.values.reshape(data.shape[0], 1)\n",
    "summaryAndCount = data.summaryAndCount.values.reshape(data.shape[0], 1)\n",
    "summaryQuestionCount = data.summaryQuestionCount.values.reshape(data.shape[0], 1)\n",
    "summaryAsterickCount = data.summaryAsterickCount.values.reshape(data.shape[0], 1)\n",
    "summaryFParenthesisCount = data.summaryFParenthesisCount.values.reshape(data.shape[0], 1)\n",
    "summaryBParenthesisCount = data.summaryBParenthesisCount.values.reshape(data.shape[0], 1)\n",
    "# summaryPunctuationCount = data.summaryPunctuationCount.values.reshape(data.shape[0], 1)\n",
    "# summaryPunctuationToWords = data.iloc[:, 70].values.reshape(data.shape[0], 1)\n",
    "# summaryRepeatTxt = data.iloc[:, 71].values.reshape(data.shape[0], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>...</th>\n",
       "      <th>summarySemiCount</th>\n",
       "      <th>summaryAtCount</th>\n",
       "      <th>summaryDollaCount</th>\n",
       "      <th>summaryNumbCount</th>\n",
       "      <th>summaryAndCount</th>\n",
       "      <th>summaryQuestionCount</th>\n",
       "      <th>summaryAsterickCount</th>\n",
       "      <th>summaryFParenthesisCount</th>\n",
       "      <th>summaryBParenthesisCount</th>\n",
       "      <th>summaryRepeatTxt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138806</td>\n",
       "      <td>138807</td>\n",
       "      <td>B000E63LME</td>\n",
       "      <td>A1CQGW1AOD0LF2</td>\n",
       "      <td>Alena K. \"Alena\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1294185600</td>\n",
       "      <td>Not as pictured.</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>469680</td>\n",
       "      <td>469681</td>\n",
       "      <td>B004ZIH4KM</td>\n",
       "      <td>A37S7U1OX2MCWI</td>\n",
       "      <td>Becky Cole</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1349740800</td>\n",
       "      <td>seeds</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238202</td>\n",
       "      <td>238203</td>\n",
       "      <td>B003ZXE9QA</td>\n",
       "      <td>A2OM6G73E64EQ9</td>\n",
       "      <td>jeff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1329264000</td>\n",
       "      <td>I'm addicted!</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>485307</td>\n",
       "      <td>485308</td>\n",
       "      <td>B001RVFERK</td>\n",
       "      <td>A25W349EE97NBK</td>\n",
       "      <td>Tangent4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1248307200</td>\n",
       "      <td>I wanted to love these...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375283</td>\n",
       "      <td>375284</td>\n",
       "      <td>B000OQZNTS</td>\n",
       "      <td>A3CPPW0HUC07YS</td>\n",
       "      <td>Amy Nicolai</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1333238400</td>\n",
       "      <td>Excellent chamomile tea</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Id   ProductId          UserId       ProfileName  \\\n",
       "0      138806  138807  B000E63LME  A1CQGW1AOD0LF2  Alena K. \"Alena\"   \n",
       "1      469680  469681  B004ZIH4KM  A37S7U1OX2MCWI        Becky Cole   \n",
       "2      238202  238203  B003ZXE9QA  A2OM6G73E64EQ9              jeff   \n",
       "3      485307  485308  B001RVFERK  A25W349EE97NBK          Tangent4   \n",
       "4      375283  375284  B000OQZNTS  A3CPPW0HUC07YS       Amy Nicolai   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       2      2  1294185600   \n",
       "1                     0                       0      5  1349740800   \n",
       "2                     0                       0      5  1329264000   \n",
       "3                     1                       1      4  1248307200   \n",
       "4                     0                       0      5  1333238400   \n",
       "\n",
       "                     Summary       ...        summarySemiCount  \\\n",
       "0           Not as pictured.       ...                       0   \n",
       "1                      seeds       ...                       0   \n",
       "2              I'm addicted!       ...                       0   \n",
       "3  I wanted to love these...       ...                       0   \n",
       "4    Excellent chamomile tea       ...                       0   \n",
       "\n",
       "   summaryAtCount summaryDollaCount summaryNumbCount summaryAndCount  \\\n",
       "0               0                 1                0               0   \n",
       "1               0                 1                0               0   \n",
       "2               0                 1                0               0   \n",
       "3               0                 1                0               0   \n",
       "4               0                 1                0               0   \n",
       "\n",
       "  summaryQuestionCount summaryAsterickCount summaryFParenthesisCount  \\\n",
       "0                    0                    0                        0   \n",
       "1                    0                    0                        0   \n",
       "2                    0                    0                        0   \n",
       "3                    0                    0                        0   \n",
       "4                    0                    0                        0   \n",
       "\n",
       "  summaryBParenthesisCount summaryRepeatTxt  \n",
       "0                        0            False  \n",
       "1                        0            False  \n",
       "2                        0            False  \n",
       "3                        0            False  \n",
       "4                        0            False  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Xtoadd = np.concatenate((\n",
    "#                          XScore, \n",
    "                         XhasEP, \n",
    "                         XhasSemi, \n",
    "                         XhasColon, \n",
    "                         hasAt, \n",
    "                         hasDolla, \n",
    "                         hasNumb, \n",
    "                         hasAnd, \n",
    "                         hasQuestion, \n",
    "                         hasAsterick, \n",
    "                         hasFparenthesis, \n",
    "                         hasBparenthesis, \n",
    "                         epCount, \n",
    "                         semiCount, \n",
    "                         atCount, \n",
    "                         dollaCount, \n",
    "                         numbCount, \n",
    "                         andCount, \n",
    "                         questionCount, \n",
    "                         asterickCount, \n",
    "                         fParenthesisCount, \n",
    "                         bParenthesisCount, \n",
    "#                          punctuationCount, \n",
    "#                          punctuationToWords, \n",
    "                         repeatTxt, \n",
    "                         nameNumb, \n",
    "                         nameQuestion, \n",
    "                         nameAsterick, \n",
    "                         nameAnd, \n",
    "                         nameSemi, \n",
    "                         nameColon, \n",
    "                         nameAt, \n",
    "                         nameDolla, \n",
    "#                          repeatProfile, \n",
    "#                          summaryLen, \n",
    "                         summaryEP, \n",
    "                         summarySemi, \n",
    "                         summaryColon, \n",
    "                         summaryAt, \n",
    "                         summaryDolla, \n",
    "                         summaryNumb, \n",
    "                         summaryAnd, \n",
    "                         summaryQuestion, \n",
    "                         summaryAsterick, \n",
    "                         summaryFparenthesis, \n",
    "                         summaryBparenthesis, \n",
    "                         summaryEpCount, \n",
    "                         summarySemiCount, \n",
    "                         summaryAtCount, \n",
    "                         summaryDollaCount, \n",
    "                         summaryNumbCount, \n",
    "                         summaryAndCount, \n",
    "                         summaryQuestionCount, \n",
    "                         summaryAsterickCount, \n",
    "                         summaryFParenthesisCount, \n",
    "                         summaryBParenthesisCount \n",
    "#                          summaryPunctuationCount, \n",
    "#                          summaryPunctuationToWords, \n",
    "#                          summaryRepeatTxt\n",
    "    ), axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xfeatures = np.concatenate(( X_counts, Xtoadd), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y = data.iloc[:, 12].values\n",
    "# y = np.where(y == 'true', -1, 1)\n",
    "\n",
    "# X = data.iloc[:, 35].values #punctuation count\n",
    "\n",
    "# plt.scatter(X[:50, 0], X[:50, 1],\n",
    "# ... color='red', marker='o', label='helpfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# report on training and test sets\n",
    "global SVMerror, SVMacc, SVMtp, SVMtn, LRerror, LRacc, LRtp, LRtn, NBerror, NBacc, NBtp, NBtn, Perror, Pacc, Ptp, Ptn\n",
    "\n",
    "\n",
    "\n",
    "def print_results(model):\n",
    "    #print('Error rate on training set: ')\n",
    "    erTRAIN = ((y_train != y_pred).sum() / X_train.shape[0])\n",
    "    #print('Accuracy rate on training set: ')\n",
    "    AccTRAIN = (1 - (y_train != y_pred).sum() / X_train.shape[0])\n",
    "    #print('True positive rate on training tet:')\n",
    "    TruPosTRAIN = ((y_train==True) & (y_pred==True)).sum() / y_train.sum()\n",
    "    #TruNegTEST = (((y_train==False) & (y_pred_train==False)).sum() / (y_train.shape[0] - y_train.sum()))\n",
    "    #print('**************')\n",
    "    #('Error rate on test set: ')\n",
    "    erTEST = ((y_test != y_pred_test).sum() / X_test.shape[0])\n",
    "    #print('Accuracy rate on test set: ')\n",
    "    AccTEST = (1 - (y_test != y_pred_test).sum() / X_test.shape[0])\n",
    "    #print('True positive rate on test set')\n",
    "    TruPosTEST = (((y_test==True) & (y_pred_test==True)).sum() / y_test.sum())\n",
    "    #print('True negative rate on test set')\n",
    "    TruNegTEST = (((y_test==False) & (y_pred_test==False)).sum() / (y_test.shape[0] - y_test.sum()))\n",
    "    data_rows = [('Error Rate', erTRAIN, erTEST),\n",
    "                 ('Accuracy Rate', AccTRAIN, AccTEST),\n",
    "                 ('True Positives', TruPosTRAIN, TruPosTEST),\n",
    "                 ('True Negatives', '--', TruNegTEST)]\n",
    "    t = Table(rows=data_rows, names=(model, 'Training Set', 'Test Set'), meta={'name': model + ': Training and Test Set Results'})\n",
    "    print(t)\n",
    "    if model == 'SVM':\n",
    "        SVMerror = erTEST\n",
    "        SVMacc = AccTEST\n",
    "        SVMtp = TruPosTEST\n",
    "        SVMtn = TruNegTEST\n",
    "        return(SVMerror, SVMacc, SVMtp, SVMtn)\n",
    "    elif model == 'Logistic Regression':\n",
    "        LRerror = erTEST\n",
    "        LRacc = AccTEST\n",
    "        LRtp = TruPosTEST\n",
    "        LRtn = TruNegTEST\n",
    "        return(LRerror, LRacc, LRtp, LRtn)\n",
    "    elif model == 'Naive Bayes':\n",
    "        NBerror = erTEST\n",
    "        NBacc = AccTEST\n",
    "        NBtp = TruPosTEST\n",
    "        NBtn = TruNegTEST\n",
    "        return(NBerror, NBacc, NBtp, NBtn)\n",
    "    elif model == 'Perceptron':\n",
    "        Perror = erTEST\n",
    "        Pacc = AccTEST\n",
    "        Ptp = TruPosTEST\n",
    "        Ptn = TruNegTEST\n",
    "        return(Perror, Pacc, Ptp, Ptn)\n",
    "    print('done')\n",
    "      \n",
    "    #t.show_in_browser(jsviewer=True) \n",
    "    \n",
    "    \n",
    "def all_models_table():\n",
    "    all_rows = [('SVM', SVMerror, SVMacc, SVMtp, SVMtn),\n",
    "            ('Logistic Regression', LRerror, LRacc, LRtp, LRtn),\n",
    "            ('Naive Bayes', NBerror, NBacc, NBtp, NBtn),\n",
    "            ('Perceptron', Perror, Pacc, Ptp, Ptn)]\n",
    "    tt = Table(rows=all_rows, names=('', 'Error Rate', 'Accuracy', 'True +', 'True -'), meta={'3/15/2016'})\n",
    "    print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('O'),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a3de275fbb2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert to CSR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mXtoaddSparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtoadd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mXfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXtoadd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtoaddSparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXhashText\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXhashSummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXText\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXSummary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     67\u001b[0m                         self.format)\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Read matrix dimensions given, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36masformat\u001b[0;34m(self, format)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'to'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m###################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             coo_tocsr(M, N, self.nnz,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no supported conversion for types: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)"
     ]
    }
   ],
   "source": [
    "# convert to CSR\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "XtoaddSparse = csr_matrix(Xtoadd)\n",
    "Xfinal = hstack([Xtoadd, XtoaddSparse,XhashText,XhashSummary,XText,XSummary])\n",
    "X = csr_matrix(Xfinal)\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "X = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455000,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define y\n",
    "# y = data.iloc[:, 12].values\n",
    "# y.shape\n",
    "y = data.helpful.values\n",
    "y.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training and test sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# decision tree classifier\n",
    "\n",
    "# from sklearn import tree\n",
    "# X = [[0, 0], [1, 1]]\n",
    "# Y = [0, 1]\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SVM          Training Set      Test Set   \n",
      "-------------- ----------------- --------------\n",
      "    Error Rate 0.000863422291994 0.152827838828\n",
      " Accuracy Rate    0.999136577708 0.847172161172\n",
      "True Positives    0.993972791459 0.446087738583\n",
      "True Negatives                -- 0.878902389856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.15282783882783882,\n",
       " 0.84717216117216121,\n",
       " 0.44608773858299189,\n",
       " 0.87890238985556512)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL: SVM, linear\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(X_train_std, y_train)\n",
    "y_pred = clf.fit(X_train_std, y_train).predict(X_train_std)\n",
    "y_pred_test = clf.predict(X_test_std)\n",
    "print_results('SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression   Training Set      Test Set   \n",
      "------------------- ---------------- --------------\n",
      "         Error Rate 0.00110518053375 0.145465201465\n",
      "      Accuracy Rate   0.998894819466 0.854534798535\n",
      "     True Positives   0.994962975719 0.438892775057\n",
      "     True Negatives               -- 0.887416694995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.14546520146520148,\n",
       " 0.85453479853479852,\n",
       " 0.43889277505745977,\n",
       " 0.88741669499497999)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL: logistic regression\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(loss='log', n_iter=50, alpha=0.00001)\n",
    "clf.fit(X_train_std, y_train)\n",
    "y_pred = clf.fit(X_train_std, y_train).predict(X_train_std)\n",
    "y_pred_test = clf.predict(X_test_std)\n",
    "print_results('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Naive Bayes     Training Set       Test Set   \n",
      "-------------- ---------------- ---------------\n",
      "    Error Rate 0.00194662480377 0.0805054945055\n",
      " Accuracy Rate   0.998053375196  0.919494505495\n",
      "True Positives   0.999354227656  0.328969721195\n",
      "True Negatives               --   0.96621156902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.080505494505494504,\n",
       " 0.91949450549450551,\n",
       " 0.32896972119516338,\n",
       " 0.96621156901962957)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL: Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_std, y_train)\n",
    "y_pred = clf.fit(X_train_std, y_train).predict(X_train_std)\n",
    "y_pred_test = clf.predict(X_test_std)\n",
    "print_results('Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Perceptron      Training Set      Test Set   \n",
      "-------------- ----------------- --------------\n",
      "    Error Rate 0.000875981161695 0.152564102564\n",
      " Accuracy Rate    0.999124018838 0.847435897436\n",
      "True Positives    0.994015842948 0.445188368142\n",
      "True Negatives                -- 0.879258140767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.15256410256410258,\n",
       " 0.84743589743589742,\n",
       " 0.44518836814230039,\n",
       " 0.87925814076668274)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perceptron\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(loss='perceptron')\n",
    "clf.fit(X_train_std, y_train)\n",
    "y_pred = clf.fit(X_train_std, y_train).predict(X_train_std)\n",
    "y_pred_test = clf.predict(X_test_std)\n",
    "print_results('Perceptron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
